# 决策树

## 基本概念

1. 信息增益

  特征$A$对训练数据集$D$的信息增益$g(D,A)$，定义为集合$D$的经验熵$H(D)$（随机变量为样本的**类别**）与特征$A$给定条件下$D$的经验条件熵$H(D|A)$之差
  $$
  g(D,A)=H(D)-H(D|A)
  $$
  经验熵$H(D)$表示对数据集$D$进行分类的不确定性，
  经验条件熵$H(D|A)$表示在特征$A$给定的条件下对数据集$D$进行分类的不确定性，
  那么他们的差，即信息增益，表示由于特征$A$而使得数据集$D$的分类的不确定性减少的程度
  **缺陷**：以信息增益作为划分训练数据集的特征，存在偏向于选择**取值较多的特征**的问题。（考虑极端情况，若$A$将$D$中的每个样本作为一个子集，则$H(D|A)=0$，则该特征对应的信息增益最大）

2. 信息增益比

  特征$A$对训练数据集$D$的信息增益比$g_r(D,A)$定义为其信息增益$g(D,A)$与**训练数据集$D$关于特征$A$的取值的熵$H_A(D)$**之比
  其中，$H_A(D)=-\sum_{i=1}^n P(A=a_i)\log P(A=a_i)$，$n$为特征$A$的取值个数，$P(A=a_i)$表示数据集中特征$A$取值为$a_i$的样本频率。

3. 基尼指数

  分类问题中，假设有$K$个类，样本点属于第$k$类的概率为$p_k$，则概率分布的基尼指数定义为
  $$
  Gini(p)=\sum_{k=1}^K p_k(1-p_k)=1-\sum_{k=1}^K p_k^2
  $$
  对于给定样本集合$D$，其基尼指数为
  $$
  Gini(D)=1-\sum_{k=1}^K(\frac {|C_k|}{|D})^2
  $$
  其中，$C_k$是$D$中属于第$k$类的样本子集。

## ID3

**决策树学习的基本算法**

**输入**：训练集$D$，特征集$A$，阈值$\varepsilon$

**输出**：决策树$T$

**过程**：函数$TreeGenerate(D,A)$

1. 生成节点node;
2. **if** $D$中样本全主语同一类别$C_k$
3. 　　将node标记为类别为$C_k$的叶节点; **return**
4. **end if**
5. **if** $A=\varnothing$ **or** $D$中样本在$A$上取值相同
6. 　　将node标记为叶节点，其类别为$D$中包含样本数最多的类; **return**
7. **end if**
8. 从$A$中选择最优划分属性$a^*$;
9. **for** $a^*$的每一种取值$a^*_i$ **do**
10. 　　为node生成一个分支；令$D_i$表示$D$中在$a^*$属性上取值为$a^*_i$的样本子集;
11. 　　**if** $Di=\varnothing$
12. 　　　　将分支节点标记为叶节点，其类别标记为$D$中样本最多的类; **return**
13. 　　**else**
14. 　　　　以$TreeGenerate(D_i, A\backslash \{a^*\})$为分支节点
15. 　　**end if**
16. **end for**

## C4.5



## CART



