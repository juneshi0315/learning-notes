{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 信息论基础\n",
    "\n",
    "### 熵\n",
    "\n",
    "表示随机变量的不确定程度。熵越大，随机变量不确定性就越大，其信息量就越大。\n",
    "设X是一个离散随机变量（如果是连续随机变量，熵无穷大，只能计算相对熵），其概率分布为\n",
    "$$\n",
    "P(X=x_i)=p_i,　i=1,2,...,n\n",
    "$$\n",
    "则随机变量的熵定义为\n",
    "$$\n",
    "H(X)=-\\sum_{i} p_i\\log p_i\n",
    "$$\n",
    "可看作$$-\\log P(X=x_i)$$的期望。\n",
    "\n",
    "### 条件熵\n",
    "\n",
    "$H(Y|X)$表示在已知随机变量$X$的条件下，随机变量$Y$的不确定性，定义为$X$给定的条件下$Y$的条件概率分布的熵对$X$的数学期望，即$X$给定的条件下$Y$的平均信息量。\n",
    "$$\n",
    "\\begin{align}\n",
    "H(Y|X)&=\\sum_{x} P(X=x)\\cdot H(Y|X=x) \\\\\n",
    "&=\\sum_{x} P(X=x)\\cdot (-\\sum_{y} P(Y=y|X=x)\\log P(Y=y|X=x)) \\\\\n",
    "&=-\\sum_{x} \\sum_{y} P(X=x, Y=y) \\cdot \\log P(Y=y|X=x) \\\\\n",
    "&=-\\sum_{x} \\sum_{y} P(X=x, Y=y) \\cdot \\log \\frac {P(X=x, Y=y)}{P(X=x)}) \\\\\n",
    "&=-\\sum_{x} \\sum_{y} P(X=x, Y=y) \\cdot \\log P(X=x,Y=y)+\\sum_{x} \\sum_{y} P(X=x, Y=y)\\cdot \\log P(X=x) \\\\\n",
    "&=-\\sum_{x} \\sum_{y} P(X=x, Y=y) \\cdot \\log P(X=x,Y=y)+\\sum_{x} P(X=x)\\cdot \\log P(X=x )\\\\\n",
    "&=H(X,Y)-H(X)\n",
    "\\end{align}\n",
    "$$\n",
    "条件熵满足$H(X,Y)=H(X)+H(Y|X)=H(Y)+H(X|Y)$，类似地，条件概率满足$P(X,Y)=P(X)\\cdot P(Y|X)=P(Y)\\cdot P(X|Y)$，因为熵是概率的对数形式。\n",
    "当熵和条件熵由数据估计得到时，熵和条件熵分别称为**经验熵**和**经验条件熵**\n",
    "\n",
    "### 互信息\n",
    "\n",
    "互信息表示随机变量间**相互依赖的程度**(即相关性，与**相关系数**的区别？)。两个离散随机变量$X$和$Y$的互信息定义为：\n",
    "$$\n",
    "I(X;Y)=\\sum_{y} \\sum_{x} p(x, y) \\log \\frac{p(x,y)}{p(x) \\cdot p(y)}\n",
    "$$\n",
    "连续随机变量互信息将求和替换为积分即可。\n",
    "\n",
    "互信息又可以等价地表示为(参照下图理解)\n",
    "$$\n",
    "\\begin{align}\n",
    "I(X;Y) &= H(X) - H(X|Y) \\\\\n",
    "&= H(Y) - H(Y|X) \\\\\n",
    "&= H(X) + H(Y) -H(X,Y) \\\\\n",
    "&= H(X,Y) -H(X|Y) - H(Y|X)\n",
    "\\end{align}\n",
    "$$\n",
    "即互信息可理解为已知$Y$的前提下，为消除$X$的不确定性所提供的信息量。\n",
    "\n",
    "此外，互信息具有非负性，对称性；$0 \\le I(X;Y) \\le \\min (H(X), H(Y))$；当$X$和$Y$独立时，$I(X;Y)=0$；\n",
    "\n",
    "![img](http://image.sciencenet.cn/album/201607/07/090617u7nu8paman1tjm5s.jpg)\n",
    "\n",
    "### 相对熵/交叉熵/KL散度(Relative Entropy/Cross Entropy/Kullback-Leibler Divergence)\n",
    "\n",
    "用来衡量两个取值为正的函数的相似性，定义如下\n",
    "$$\n",
    "KL(f(x) || g(x)) = \\sum_{x \\in X} f(x) \\cdot \\log \\frac{f(x)}{g(x)}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
